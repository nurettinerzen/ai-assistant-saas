/**
 * Step 5: Build LLM Request
 *
 * - Applies tool gating policy
 * - Builds Gemini request with gated tools
 * - Returns chat session and request configuration
 */

import { GoogleGenerativeAI } from '@google/generative-ai';
import { applyToolGatingPolicy } from '../../../policies/toolGatingPolicy.js';
import { convertToolsToGeminiFunctions as convertToolsToGemini } from '../../../services/gemini-utils.js';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

export async function buildLLMRequest(params) {
  const {
    systemPrompt,
    conversationHistory,
    userMessage,
    classification,
    routingResult,
    state,
    toolsAll,
    metrics,
    assistant,
    business
  } = params;

  // STEP 0: Enhance system prompt with known customer info
  // SECURITY: Only send non-PII identifiers to LLM, not actual customer data
  let enhancedSystemPrompt = systemPrompt;
  if (state.extractedSlots && Object.keys(state.extractedSlots).length > 0) {
    const knownInfo = [];
    // Only include identifiers, not actual PII values
    if (state.extractedSlots.customer_name) {
      knownInfo.push(`Customer name mentioned`);
    }
    if (state.extractedSlots.phone) {
      knownInfo.push(`Phone number provided`);
    }
    if (state.extractedSlots.order_number) {
      knownInfo.push(`Order #${state.extractedSlots.order_number}`); // Order number is OK
    }
    if (state.extractedSlots.email) {
      knownInfo.push(`Email mentioned`);
    }

    if (knownInfo.length > 0) {
      enhancedSystemPrompt += `\n\nCustomer Context: ${knownInfo.join(', ')} - Use tools to retrieve actual data`;
      console.log('ðŸ“ [BuildLLMRequest] Added context flags (no PII):', knownInfo.length, 'indicators');
    }
  }

  // ========================================
  // ARCHITECTURE CHANGE: Inject verification & dispute context for LLM
  // ========================================
  // LLM now handles verification conversation naturally.
  // We inject context so it knows what's pending.
  if (state.verificationContext) {
    const vc = state.verificationContext;
    const verificationGuidance = `

## DOÄžRULAMA DURUMU (Verification Context)
- Durum: ${vc.status}
- Beklenen bilgi: ${vc.pendingField === 'name' ? 'Ad-soyad' : vc.pendingField === 'phone' ? 'Telefon numarasÄ±' : vc.pendingField}
- Deneme sayÄ±sÄ±: ${vc.attempts}/3

KURALLAR:
- KullanÄ±cÄ±nÄ±n son mesajÄ±nÄ± baÄŸlam iÃ§inde yorumla
- EÄŸer kullanÄ±cÄ± doÄŸrulama bilgisi verdiyse, customer_data_lookup tool'unu verification_input parametresiyle Ã§aÄŸÄ±r
- EÄŸer kullanÄ±cÄ± farklÄ± bir soru sorduysa, soruyu cevapla ama doÄŸrulama ihtiyacÄ±nÄ± da hatÄ±rlat
- AynÄ± cÃ¼mleyi tekrar etme â€” her seferinde farklÄ± ve doÄŸal konuÅŸ
- YanlÄ±ÅŸ anladÄ±ÄŸÄ±nÄ± fark edersen "SanÄ±rÄ±m bir karÄ±ÅŸÄ±klÄ±k oldu..." diyebilirsin
- "LÃ¼tfen ad-soyadÄ±nÄ±zÄ± yazÄ±nÄ±z" gibi form cÃ¼mleleri KULLANMA`;

    enhancedSystemPrompt += verificationGuidance;
    console.log('ðŸ” [BuildLLMRequest] Added verification context for LLM');

    // Clean up - don't persist this context
    delete state.verificationContext;
  }

  // Dispute context â€” LLM has anchor/truth data to reference
  if (state.disputeContext) {
    const dc = state.disputeContext;
    const disputeGuidance = `

## Ä°TÄ°RAZ BAÄžLAMI (Dispute Context)
KullanÄ±cÄ± Ã¶nceki sonucu reddediyor/itiraz ediyor.
- Ã–nceki akÄ±ÅŸ: ${dc.originalFlow || 'bilinmiyor'}
- Kargo takip bilgisi var mÄ±: ${dc.hasTrackingInfo ? 'EVET' : 'HAYIR'}

KURALLAR:
- KullanÄ±cÄ±nÄ±n itirazÄ±nÄ± ciddiye al
- Elindeki bilgileri (varsa kargo takip no) doÄŸal dille paylaÅŸ
- Geri arama teklif et
- Empati kur, "ama sistem ÅŸunu sÃ¶ylÃ¼yor" gibi savunmacÄ± olma`;

    enhancedSystemPrompt += disputeGuidance;
    console.log('âš ï¸ [BuildLLMRequest] Added dispute context for LLM');

    // Clean up
    delete state.disputeContext;
  }

  // Profanity strike context â€” LLM handles warning naturally
  if (routingResult?.routing?.routing?.profanityStrike) {
    const strike = routingResult.routing.routing.profanityStrike;
    const profanityGuidance = `

## KÃœFÃœR UYARISI
KullanÄ±cÄ± saygÄ±sÄ±z dil kullandÄ± (${strike}. uyarÄ± / 3 Ã¼zerinden).
- Kibarca uyar ama suÃ§lama
- YardÄ±m etmeye devam et
- DoÄŸal ve empatik ol`;

    enhancedSystemPrompt += profanityGuidance;
    console.log(`âš ï¸ [BuildLLMRequest] Added profanity context (strike ${strike}/3)`);
  }

  // STEP 0.5: CHATTER messages â€” CONTEXT-PRESERVING PROMPT
  // When chatterDirective is present (LLM mode), use directive-driven prompt.
  // Otherwise (legacy direct template mode that reached here), use generic chatter guidance.
  const isChatterRoute = routingResult?.isChatter || routingResult?.routing?.routing?.action === 'ACKNOWLEDGE_CHATTER';
  const chatterDirective = routingResult?.chatterDirective;

  if (chatterDirective) {
    // â”€â”€ LLM directive mode (flag ON) â”€â”€
    const assistantName = assistant?.name || 'Asistan';
    const businessName = business?.name || '';

    enhancedSystemPrompt += `

## CHATTER KISA YANIT MODU (LLM Directive)
- RolÃ¼n: ${businessName ? businessName + ' ÅŸirketinin' : 'ÅŸirketin'} mÃ¼ÅŸteri asistanÄ± ${assistantName}
- Mesaj tÃ¼rÃ¼: ${chatterDirective.kind} (greeting/thanks/generic)
- KonuÅŸma durumu: ${chatterDirective.flowStatus}
- Aktif gÃ¶rev var mÄ±: ${chatterDirective.activeTask ? 'EVET â€” ' + (chatterDirective.activeFlow || 'devam eden iÅŸ') : 'HAYIR'}
- DoÄŸrulama bekleniyor mu: ${chatterDirective.verificationPending ? 'EVET' : 'HAYIR'}

KURALLAR:
- Selam/teÅŸekkÃ¼re kÄ±sa ve doÄŸal cevap ver, robotik kalÄ±p kullanma.
- Maksimum ${chatterDirective.maxSentences} cÃ¼mle yaz.
- "Size nasÄ±l yardÄ±mcÄ± olabilirim?" veya benzer kliÅŸe yardÄ±m cÃ¼mlelerini TEKRARLAMA.
- EÄŸer aktif gÃ¶rev varsa, kÄ±sa yanÄ±t sonrasÄ± gÃ¶reve nazikÃ§e geri dÃ¶n.
- KullanÄ±cÄ± net bir talep vermediyse tek cÃ¼mlelik sÄ±cak bir karÅŸÄ±lÄ±k ver.

TON KISITLAMALARI:
- SatÄ±ÅŸ dili kullanma (no_salesy). "Harika fÄ±rsatlar", "kaÃ§Ä±rma" gibi ifadeler YASAK.
- Garip veya aÅŸÄ±rÄ± samimi selamlaÅŸmalardan kaÃ§Ä±n (no_weird_greetings). "CanÄ±m mÃ¼ÅŸterim", "tatlÄ±m" gibi ifadeler YASAK.
- AÅŸÄ±rÄ± dostane/informal olma (no_overfriendly). Profesyonel ama sÄ±cak bir ton koru.
- Ã–nceki selamlaÅŸmayÄ± birebir tekrarlama, ama tutarlÄ± bir ton ve Ã¼slup koru.`;
    console.log('ðŸ’¬ [BuildLLMRequest] CHATTER â€” LLM directive mode active');
  } else if (isChatterRoute) {
    // â”€â”€ Legacy mode (flag OFF, but reached LLM for some reason) â”€â”€
    const assistantName = assistant?.name || 'Asistan';
    const businessName = business?.name || '';
    const activeFlowSummary = state.activeFlow || state.flowStatus || 'none';
    const hasPendingVerification = state.verification?.status === 'pending';

    enhancedSystemPrompt += `

## CHATTER KISA YANIT MODU
- RolÃ¼n: ${businessName ? businessName + ' ÅŸirketinin' : 'ÅŸirketin'} mÃ¼ÅŸteri asistanÄ± ${assistantName}
- KonuÅŸma durumu: ${activeFlowSummary}
- DoÄŸrulama bekleniyor mu: ${hasPendingVerification ? 'EVET' : 'HAYIR'}

KURALLAR:
- Selam/teÅŸekkÃ¼re kÄ±sa ve doÄŸal cevap ver, robotik kalÄ±p kullanma.
- EÄŸer konuÅŸmada aktif bir gÃ¶rev varsa (Ã¶r: sipariÅŸ, doÄŸrulama), kÄ±sa yanÄ±t sonrasÄ± gÃ¶reve nazikÃ§e geri dÃ¶n.
- "Size nasÄ±l yardÄ±mcÄ± olabilirim?" cÃ¼mlesini her selamda tekrarlama.
- KullanÄ±cÄ± net bir talep vermediyse tek cÃ¼mlelik sÄ±cak bir karÅŸÄ±lÄ±k ver.`;
    console.log('ðŸ’¬ [BuildLLMRequest] CHATTER â€” context-preserving guidance aktif');
  }

  // STEP 1: Apply tool gating policy
  const classifierConfidence = classification?.confidence || 0.9;

  // OPTIMIZATION: Skip tools entirely for CHATTER messages (greetings, acknowledgments)
  // This saves ~5000 tokens per CHATTER turn and reduces latency
  const isChatter = routingResult?.isChatter || routingResult?.routing?.routing?.action === 'ACKNOWLEDGE_CHATTER';

  let gatedTools;
  if (isChatter) {
    gatedTools = [];
    console.log('ðŸ’¬ [BuildLLMRequest] CHATTER detected â€” skipping all tools (0 token overhead)');
  } else {
    // If no flow-specific tools, use ALL available tools (extract names from toolsAll)
    const allToolNames = toolsAll.map(t => t.function?.name).filter(Boolean);
    console.log('ðŸ”§ [BuildLLMRequest] toolsAll:', { count: toolsAll.length, names: allToolNames });

    const flowTools = (state.allowedTools && state.allowedTools.length > 0)
      ? state.allowedTools
      : allToolNames;

    gatedTools = applyToolGatingPolicy({
      confidence: classifierConfidence,
      activeFlow: state.activeFlow,
      allowedTools: flowTools,
      verificationStatus: state.verificationStatus,
      metrics
    });

    console.log('ðŸ”§ [BuildLLMRequest]:', {
      originalTools: flowTools.length,
      gatedTools: gatedTools.length,
      confidence: classifierConfidence.toFixed(2),
      removed: flowTools.filter(t => !gatedTools.includes(t))
    });
  }

  // STEP 2: Filter tools based on gated list
  // toolsAll is in OpenAI format: {type: 'function', function: {name, description, parameters}}
  const allowedToolObjects = toolsAll.filter(tool =>
    gatedTools.includes(tool.function?.name)
  );

  // STEP 3: Convert tools to Gemini format
  const geminiTools = allowedToolObjects.length > 0
    ? convertToolsToGemini(allowedToolObjects)
    : [];

  // STEP 4: Build conversation history for Gemini
  const geminiHistory = conversationHistory.map(msg => ({
    role: msg.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: msg.content }]
  }));

  // STEP 5: Create Gemini chat session
  // Chatter-specific budget: lower tokens + temperature for cost/latency savings
  const isChatterLLM = !!chatterDirective;
  const generationConfig = isChatterLLM
    ? {
        temperature: 0.5,
        topP: 0.95,
        topK: 40,
        maxOutputTokens: 200,
        thinkingConfig: { thinkingBudget: 0 }
      }
    : {
        temperature: 0.7,
        topP: 0.95,
        topK: 40,
        maxOutputTokens: 1024,
        thinkingConfig: { thinkingBudget: 0 }
      };

  if (isChatterLLM) {
    console.log('ðŸ’¬ [BuildLLMRequest] CHATTER budget: maxOutputTokens=200, temperature=0.5');
  }

  const model = genAI.getGenerativeModel({
    model: 'gemini-2.5-flash',
    systemInstruction: enhancedSystemPrompt,
    tools: geminiTools.length > 0 ? [{ functionDeclarations: geminiTools }] : undefined,
    toolConfig: geminiTools.length > 0 ? {
      functionCallingConfig: {
        mode: 'AUTO'
      }
    } : undefined,
    generationConfig
  });

  const chat = model.startChat({
    history: geminiHistory
  });

  // STEP 6: Update state with gated tools
  state.allowedTools = gatedTools;

  return {
    chat,
    gatedTools,
    hasTools: gatedTools.length > 0,
    model,
    confidence: classifierConfidence
  };
}

export default { buildLLMRequest };
